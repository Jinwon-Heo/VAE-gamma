{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acoustic-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "from torchvision import datasets\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from VAE_gamma import BasicDataset, generate_data, VAE_gamma, pretrain_fit, train_model, cluster_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "approximate-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "delta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efficient-rubber",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_all = generate_data(n,dim=10, set_1 = [5, 1, 10], set_2 = [15, 2, 10], delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "completed-azerbaijan",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_1 = torch.zeros(n).int()\n",
    "label_2 = torch.ones(n).int()\n",
    "\n",
    "label_all = torch.cat([label_1,label_2],dim=0)\n",
    "\n",
    "train_dataset = BasicDataset(data_all, label_all)\n",
    "test_dataset = BasicDataset(data_all, label_all)\n",
    "\n",
    "batch_size = int(n/10)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ambient-tracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurasy: 90.0\n",
      "ARI: 0.6382285926668068\n",
      "***** Test Result: loss: 0.828 acc: 95.5000\n",
      "Accurasy: 91.0\n",
      "ARI: 0.6707993237470232\n",
      "***** Test Result: loss: 1.266 acc: 95.5000\n",
      "Accurasy: 91.0\n",
      "ARI: 0.6707880193267941\n",
      "***** Test Result: loss: 1.299 acc: 96.5000\n",
      "Accurasy: 92.0\n",
      "ARI: 0.7041424182252666\n",
      "***** Test Result: loss: 1.234 acc: 96.0000\n",
      "Accurasy: 92.0\n",
      "ARI: 0.7041424182252666\n",
      "***** Test Result: loss: 0.746 acc: 96.5000\n"
     ]
    }
   ],
   "source": [
    "final_list = {}\n",
    "n_pre=5\n",
    "monte_number = 1\n",
    "for i in range(1, n_pre+1):      \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        pre_model =  VAE_gamma(data_dim=10, latent_dim=2, device=torch.device('cuda:5'), encodeLayer=[200,100,30],\n",
    "                               decodeLayer=[30,100,200], num_cluster=2).cuda(torch.device('cuda:5'))\n",
    "\n",
    "    optimizer_auto = optim.Adam(pre_model.parameters(), lr=0.001) \n",
    "    scheduler = StepLR(optimizer_auto, step_size=20, gamma= 0.9)\n",
    "    path_name = '/home/jupyter-heojw/Data_all/VAE-gamma/weight/simulation/pretrain'+'_'+str(i)+'.pth'\n",
    "    pretrain_epoch= 20\n",
    "\n",
    "    pretrain_fit(dataloader, pre_model, optimizer_auto,scheduler,path_name,pretrain_epoch,monte_number,2,device=torch.device('cuda:5'))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model_train = VAE_gamma(data_dim=10, latent_dim=2, device=torch.device('cuda:5'), encodeLayer=[200,100,30],\n",
    "                               decodeLayer=[30,100,200], num_cluster=2).cuda(torch.device('cuda:5'))\n",
    "    saveWeightPath = path_name\n",
    "    optimizer = optim.Adam(model_train.parameters(), lr=0.001)\n",
    "    scheduler = StepLR(optimizer, step_size=20, gamma= 0.9)\n",
    "    model_train.load_state_dict(torch.load(saveWeightPath))\n",
    "    epoch_train = 10\n",
    "\n",
    "    path_name_2 = '/home/jupyter-heojw/Data_all/VAE-gamma/weight/simulation/simulation_' + '_initial' + str(i) +  '.pth'\n",
    "\n",
    "    loss = train_model(dataloader, dataloader_test, model_train, optimizer,scheduler,epoch_train,batch_size,monte_number,10,torch.device('cuda:5'),path_name_2)\n",
    "    final_list[' initial:'+str(i) ] = loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "norman-tracker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82834107 1.26550001 1.29873805 1.23401604 0.7464927 ]\n"
     ]
    }
   ],
   "source": [
    "final_loss = np.array(list(final_list.items()))[:,1].astype('float')\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "administrative-optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ************************ sample: 100 delta: 0.1************************\n",
      "***** Test Result: loss: 0.747 acc: 96.5000\n"
     ]
    }
   ],
   "source": [
    "optim_model = np.where(final_loss==final_loss.min())[0][0] + 1 \n",
    "\n",
    "path_name_best =  '/home/jupyter-heojw/Data_all/VAE-gamma/weight/simulation/simulation_' + '_initial' + str(optim_model) +  '.pth'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model =  VAE_gamma(data_dim=10, latent_dim=2, device=torch.device('cuda:5'), encodeLayer=[200,100,30],\n",
    "                           decodeLayer=[30,100,200], num_cluster=2).cuda(torch.device('cuda:5'))\n",
    "\n",
    "saveWeightPath = path_name_best\n",
    "\n",
    "model.load_state_dict(torch.load(saveWeightPath))\n",
    "print('\\n ************************ sample: '+ str(n),'delta: '+ str(delta)+ '************************')\n",
    "model.eval()\n",
    "accuracy_list = []\n",
    "test_loss_list = []    \n",
    "test_loss_sum = 0.0\n",
    "for perform in range(0,100):\n",
    "    test_loss = 0.0\n",
    "    test_running_correct = 0.0\n",
    "    label_set_test = torch.tensor([],dtype=int).cuda(torch.device('cuda:5'))\n",
    "    preds_set_test = torch.tensor([],dtype=int).cuda(torch.device('cuda:5'))\n",
    "    for batch_idx, data in enumerate(dataloader_test):\n",
    "\n",
    "        img, label = data \n",
    "        img = img.float()\n",
    "        img = img.view(img.size(0), -1)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda(torch.device('cuda:5'))\n",
    "            label = label.cuda(torch.device('cuda:5')).float()\n",
    "\n",
    "        recon_al,recon_be, logalpha, logbeta, z = model(img)\n",
    "        monte_gamma = 0\n",
    "        monte_recon_loss = 0\n",
    "        for l in range(monte_number):\n",
    "            z_l = model.reparametrize(logalpha, logbeta)\n",
    "            m = nn.Softplus()\n",
    "            recon_al_l = 1e-6 + m(model.decode(z_l)[:,:model.data_dim])\n",
    "            recon_be_l = 1e-6 + m(model.decode(z_l)[:,model.data_dim:])\n",
    "            monte_gamma += model.get_gamma(z_l,len(dataloader_test.dataset))\n",
    "            monte_recon_loss += model.recon_loss(recon_al_l,recon_be_l, img)\n",
    "        sgvb_gamma_test = monte_gamma/monte_number\n",
    "        sgvb_loss_test = torch.mean(monte_recon_loss/monte_number \n",
    "                                    + model.kl_loss(sgvb_gamma_test, logalpha, logbeta,len(dataloader_test.dataset))) #+ 1*model.Loss_b(img,z, logalpha, logbeta, 2, latent_dim))\n",
    "\n",
    "        _, preds = torch.max(sgvb_gamma_test,1)\n",
    "\n",
    "        label_set_test = torch.cat((label_set_test,label)).int()\n",
    "        preds_set_test = torch.cat((preds_set_test,preds)).int()\n",
    "        test_loss += sgvb_loss_test.item() * img.size(0)\n",
    "\n",
    "\n",
    "    label_set_test = label_set_test.cpu()\n",
    "    preds_set_test = preds_set_test.cpu()\n",
    "\n",
    "    test_acc = cluster_acc(preds_set_test,label_set_test) * 100/preds_set_test.shape[0]\n",
    "    test_loss_sum += test_loss\n",
    "    accuracy_list = np.append(accuracy_list, test_acc)\n",
    "test_loss_list = np.append(test_loss_list, test_loss_sum/100)\n",
    "\n",
    "\n",
    "print('***** Test Result: loss: {:.3f} acc: {:.4f}'\n",
    "  .format( test_loss_list[0] / len(dataloader_test.dataset), np.max(accuracy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-ceiling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jwheo",
   "language": "python",
   "name": "jwheo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
